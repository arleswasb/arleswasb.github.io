{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2227dc25",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-10T12:51:22.702806Z",
     "iopub.status.busy": "2025-07-10T12:51:22.702544Z",
     "iopub.status.idle": "2025-07-10T12:51:26.209226Z",
     "shell.execute_reply": "2025-07-10T12:51:26.208342Z"
    },
    "papermill": {
     "duration": 3.511066,
     "end_time": "2025-07-10T12:51:26.210511",
     "exception": false,
     "start_time": "2025-07-10T12:51:22.699445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRO: O arquivo 'DB_completo.csv' não foi encontrado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib \n",
    "\n",
    "try:\n",
    "    # --- 1. Carregamento e Preparação ---\n",
    "    # AJUSTE 1: Corrigir o caminho do arquivo para o nome local.\n",
    "    file_path = 'DB_completo.csv' \n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    print(\"--- Análise Inicial do Dataset ---\")\n",
    "    print(f\"O dataset possui {df.shape[0]} linhas e {df.shape[1]} colunas.\")\n",
    "\n",
    "    df.dropna(subset=['preco_dolar'], inplace=True)\n",
    "    for col in ['developers', 'publishers']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('Desconhecido')\n",
    "\n",
    "    # --- Engenharia de Features de Tempo ---\n",
    "    print(\"\\n--- Criando features de tempo ---\")\n",
    "    df['release_year'] = df['release_year'].astype(int)\n",
    "    df['release_month'] = df['release_month'].astype(int)\n",
    "    df['release_date'] = pd.to_datetime(\n",
    "        df['release_year'].astype(str) + '-' + df['release_month'].astype(str) + '-01',\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # AJUSTE 2: Reativar a remoção de datas nulas para evitar erros.\n",
    "    df.dropna(subset=['release_date'], inplace=True)\n",
    "\n",
    "    df['ano_lancamento'] = df['release_date'].dt.year\n",
    "    df['mes_lancamento'] = df['release_date'].dt.month\n",
    "    df['trimestre'] = df['release_date'].dt.quarter\n",
    "    df['time_idx'] = (df['release_date'] - df['release_date'].min()).dt.days\n",
    "\n",
    "    # --- 2. Pré-processamento ---\n",
    "    print(\"\\n--- Iniciando Pré-processamento ---\")\n",
    "    target = 'preco_dolar'\n",
    "\n",
    "    print(\"\\n--- Agrupando categorias raras ---\")\n",
    "    min_count_publisher = 20\n",
    "    publishers_counts = df['publishers'].value_counts()\n",
    "    rare_publishers = publishers_counts[publishers_counts < min_count_publisher].index\n",
    "    df['publishers'] = df['publishers'].replace(rare_publishers, 'Outro')\n",
    "\n",
    "    min_count_developer = 20\n",
    "    developers_counts = df['developers'].value_counts()\n",
    "    rare_developers = developers_counts[developers_counts < min_count_developer].index\n",
    "    df['developers'] = df['developers'].replace(rare_developers, 'Outro')\n",
    "\n",
    "    print(\"\\n--- Criando features de popularidade ---\")\n",
    "    df['dev_popularity'] = df['developers'].map(df['developers'].value_counts())\n",
    "    df['pub_popularity'] = df['publishers'].map(df['publishers'].value_counts())\n",
    "\n",
    "    # Selecionar features e alvo\n",
    "    features = df.drop(columns=[target, 'preco_euro', 'gameid', 'title', 'release_date', 'release_year', 'release_month'])\n",
    "    \n",
    "    # AJUSTE 3: Remover 'drop_first=True' para consistência com o dashboard.\n",
    "    X_encoded = pd.get_dummies(features) \n",
    "    y = df[target]\n",
    "    print(f\"Dimensões de X após encoding: {X_encoded.shape}\")\n",
    "\n",
    "    # --- 3. Divisão dos Dados em Treino e Teste ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "    print(f\"\\nDados divididos: {len(X_train)} para treino, {len(X_test)} para teste.\")\n",
    "\n",
    "    # --- 4. Treinamento do Modelo ---\n",
    "    print(\"\\n--- Treinando o Modelo Random Forest Regressor ---\")\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1) # Reduzido n_estimators para um treino mais rápido\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    print(\"Modelo treinado com sucesso!\")\n",
    "\n",
    "    # --- 5. Avaliação do Modelo ---\n",
    "    print(\"\\n--- Avaliando a Performance do Modelo ---\")\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Erro Médio Absoluto (MAE): ${mae:.2f}\")\n",
    "    print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "    # --- 6. Importância das Variáveis (Opcional) ---\n",
    "    # (Pode manter esta seção para sua análise)\n",
    "\n",
    "    # --- 7. Salvar o Modelo e as Colunas ---\n",
    "    print(\"\\n--- Salvando o modelo e as colunas ---\")\n",
    "    joblib.dump(rf_model, 'modelo_regressao_preco.joblib')\n",
    "    joblib.dump(X_encoded.columns, 'colunas_regressao_preco.joblib')\n",
    "    print(\"Modelo de Regressão e colunas salvos com sucesso!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: O arquivo '{file_path}' não foi encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro inesperado: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7814988,
     "sourceId": 12393204,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.141268,
   "end_time": "2025-07-10T12:51:26.729331",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-10T12:51:18.588063",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
